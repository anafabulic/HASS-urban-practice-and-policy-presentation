<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>How to Mix Reality</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/gruvbox-light.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>
					<h4 class="r-fit-text">how to mix reality:</h4>
					<br>
					<p><small>an affordance-oriented framework for designing virtual spaces</small>
					<br><small>Sean Chua / 1006466</small></p>
				</section>
				<section>
					<h4>
						Why do some <em>virtual experiences</em> feel good and not others?
					</h4>
					<p class="fragment fade-in">
						(and how do we make them good?)
					</p>
				</section>
				<section data-auto-animate>
					<h4>
						Theory of affordances
					</h4>
					<p>
						When you have many options for moving through a space (<em>and the space itself communicates to you</em> the means of movement) you will feel more immersed in the space.
					</p>
					<p>
						Spatial cognition is experienced by the body; an <em>affordance</em> represents a potential avenue of movement through space.
					</p>
					<p style="font-size:0.5em;">
						Carrillo Quiroga, P. and Chacón Hernández, J.C., 2021. The Perception of Space in Virtual Reality, Correlation Between Affordances and Spatial Presence. Entreciencias: diálogos en la sociedad del conocimiento, 9(23).
					</p>
				</section>
				<section data-auto-animate>
					<h4>
						Theory of affordances
					</h4>
					<p>
						Taking this framework, we may explain why shoddily-made VR landscapes don't have mass appeal&mdash;they&rsquo;re <em>just not providing enough</em> spatial affordances.
					</p>
					<p>
						Possible link to <em>Zoom fatigue</em>&mdash;the IRL affordance of self-expression gets &lsquo;compressed&rsquo; into a flat video feed?
					</p>
					<p style="font-size:0.5em;">
						Nadler, R., 2020. Understanding &ldquo;Zoom fatigue&rdquo;: Theorizing spatial dynamics as third skins in computer-mediated communication. Computers and Composition, 58, p.102613.
					</p>
				</section>
				<section>
					<h4>But isn&rsquo;t this just a problem with <em>fidelity</em>?</h4>
					<p class="fragment fade-in">just add more pixels <span class="fragment fade-in">+ procedurally-generated textures</span> <span class="fragment fade-in">+ more processing power</span> <span class="fragment fade-in">+ better physics?</span></p>
				</section>
				<section>
					<section>
						<h3>tl;dr: <em>probably not.</em></h3>
						<p>(why?)</p>
						<p>&darr;</p>
					</section>
					<section style="text-align:left;">
						<h2>1:</h2>
						<p>Low-fidelity virtual worlds can be immersive and even lived-in&mdash;see <em>Minecraft</em> and <em>Roblox</em>.</p>
					</section>
					<section style="text-align:left;">
						<h2>2:</h2>
						<p>Despite the realism of VR games like <em>Half-Life: Alyx</em>, why do we see VR headsets at specialised arcades and not at Challenger or Popular?</p>
					</section>
				</section>
				<section>
					<h3>An <em>affordance-oriented</em> model of mixed-reality* applications</h3>
					<p><small>* virtual reality, augmented reality, and everything in between</small></p>
				</section>
				<section>
					<h4>from <em>this</em>...</h4>
					<img src="images/assumed_vr.svg">
					<p>
						<small>When the sensations are realistic, the world is realistic.</small>
					</p>
				</section>
				<section>
					<section>
						<h4>... to <em>this.</em></h4>
						<img src="images/real_vr.svg">
						<p>
							<small>When the inputs are constrained, our sensations are constrained, and our space of perceived actions are constrained.</small>
						</p>
						<p>
							&darr;
						</p>
					</section>
					<section style="text-align:left;" data-auto-animate>
						<h4>in other words:</h4>
						<p>
							<em>Different</em> interfaces take in <em>different</em> inputs (data) and produce <em>different</em> outputs (sensations).
						</p>
						<p>
							From these outputs, we sense affordances: <em>where to move and what to do</em>.
						</p>
					</section>
					<section style="text-align:left;" data-auto-animate>
						<h4>in other words:</h4>
						<p>To be as <em>rich</em> as IRL, affordances must create a sense of embodied <em>dwelling</em> + <em>serendipity</em>.</p>
						<p>We can abstract these factors as functions of <em>dwell time</em> and <em>input space</em>.</p>
					</section>
				</section>
				<section>
					<section style="text-align:left;" data-auto-animate>
						<h4>and so:</h4>
						<p>There must be a <em>large & variable input space</em> from the world to the interface.</p>
						<p><small>The &lsquo;world&rsquo;&mdash;virtual or otherwise&mdash;must provide a <em>large range of data</em> to the mixed-reality application.</small></p>
						<p><small>The mixed-reality application must produce <em>varied and continuous outputs</em> from that data.</small></p>
					</section>
					<section style="text-align:left;" data-auto-animate>
						<h4>and so:</h4>
						<p>This gives us a sense of <em>varied and continuous</em> affordances.</p>
					</section>
				</section>
				<section>
					<section style="text-align:left;" data-auto-animate>
						<h4>and so:</h4>
						<p>There must be a <em>large & variable input timescale</em> during which the user is connected to the interface.</p>
						<p><small>Think of how <em>prolonged usage</em> of your phone accustoms you to its icon layout, its quirks, its glitches.</small></p>
						<p><small>Or how a <em>prolonged stay</em> in a room gathers personal touches & artefacts.</small></p>
					</section>
					<section style="text-align:left;" data-auto-animate>
						<h4>and so:</h4>
						<p>This gives us as much time as we need to <em>dwell</em> in a mixed-reality application.</p>
					</section>
				</section>
				<section>
					<h4>types of <em>mixed-reality</em> applications in an <em>affordance-oriented</em> framework</h5>
				</section>
				<section style="text-align:left;">
					<section data-auto-animate>
						<p><span style="font-family:Libre Franklin;">type:</span> Completely virtual affordances</p>
						<p><small>(e.g. virtual realms like <em>Decentraland</em>, virtual-reality games like <em>Half-Life: Alyx</em> and <em>Superhot</em>)</small></p>
						<p><em>Inputs:</em> Internally-contained 3D environment</p>
						<p><em>Outputs:</em> Game mechanics, 3D forms, images</p>
						<p><em>Affordances:</em> Constrained to playing game (within provided rules), manipulating shapes from the 3D environ or asset pool</p>
					</section>
					<section data-auto-animate>
						<p><span style="font-family:Libre Franklin;">type:</span> Completely virtual affordances</p>
						<p><small>(e.g. virtual realms like <em>Decentraland</em>, virtual-reality games like <em>Half-Life: Alyx</em> and <em>Superhot</em>)</small></p>
						<p>elaborate on the affordance-space here</p>
					</section>
				</section>
				<section style="text-align:left;">
					<section data-auto-animate>
						<p><span style="font-family:Libre Franklin;">type:</span> Virtual-constrained affordances</p>
						<p><small>(e.g. educational aids like <em>Microsoft Hololens</em>, data-visualisation tools like <em>Flow Immersive</em>)</small></p>
						<p><em>Inputs:</em> Dataset/3D model overlaid on real-world environment</p>
						<p><em>Outputs:</em> 1-to-1 representation of dataset/3D model</p>
						<p><em>Affordances:</em> Constrained to manipulation of dataset or model, extrapolating model to real life</p>
					</section>
					<section data-auto-animate>
						<p><span style="font-family:Libre Franklin;">type:</span> Virtual-constrained affordances</p>
						<p><small>(e.g. educational aids like <em>Microsoft Hololens</em>, data-visualisation tools like <em>Flow Immersive</em>)</small></p>
						<p>elaborate on the affordance-space here</p>
					</section>
				</section>
				<section style="text-align:left;">
					<section data-auto-animate>
						<p><span style="font-family:Libre Franklin;">type:</span> Real-world-constrained affordances (1-to-1)</p>
						<p><small>(e.g. fitness apps like <em>Supernatural</em> or <em>Ghost Pacer</em>, navigation apps like <em>Google Maps AR</em>)</small></p>
						<p><em>Inputs:</em> Continuous data from your body and/or real-world geography</p>
						<p><em>Outputs:</em> 1-to-1 representation of exercise statistics </p>
						<p><em>Affordances:</em> Running, training, navigating&mdash;constrained to the actions of the body</p>
					</section>
					<section data-auto-animate>
						<p><span style="font-family:Libre Franklin;">type:</span> Real-world-constrained affordances (1-to-1)</p>
						<p><small>(e.g. fitness apps like <em>Supernatural</em> or <em>Ghost Pacer</em>, navigation apps like <em>Google Maps AR</em>)</small></p>
						<p>elaborate on the affordance-space here</p>
					</section>
				</section>
				<section style="text-align:left;">
					<section data-auto-animate>
						<p><span style="font-family:Libre Franklin;">type:</span> Real-world-constrained affordances (many-to-1)</p>
						<p><small>(e.g. measurement and translation apps like <em>Google Lens</em>, decorative apps like <em>Snapchat</em> filters)</small></p>
						<p><em>Inputs:</em> Sounds and sights from the real world</p>
						<p><em>Outputs:</em> Data (verbal or pictorial), decorations (visual and aural)</p>
						<p><em>Affordances:</em> Constrained by what you do with this data (measuring, translating, imagining, decorating)</p>
					</section>
					<section data-auto-animate>
						<p><span style="font-family:Libre Franklin;">type:</span> Real-world-constrained affordances (many-to-1)</p>
						<p><small>(e.g. measurement and translation apps like <em>Google Lens</em>, decorative apps like <em>Snapchat</em> filters)</small></p>
						<p>elaborate on the affordance-space here</p>
					</section>
				</section>
				<section style="text-align:left;">
					<section data-auto-animate>
						<p><span style="font-family:Libre Franklin;">type:</span> Real-world-constrained affordances (many-to-many)</p>
						<p><small>(e.g. the <em>internet</em> as a virtual space)</small></p>
						<p><em>Inputs:</em> Anything IRL that gets recorded</p>
						<p><em>Outputs:</em> Anything that people repost/respond to online</p>
						<p><em>Affordances:</em> Consuming, producing, reacting, remixing to content... limited to a screen.</p>
					</section>
					<section data-auto-animate>
						<p><span style="font-family:Libre Franklin;">type:</span> Real-world-constrained affordances (many-to-many)</p>
						<p><small>(e.g. the <em>internet</em> as a virtual space)</small></p>
						<p>elaborate on the affordance-space here</p>
					</section>
				</section>
				<section>
					<h4>tl;dr</h4>
					<p>(insert table)</p>
				</section>
				<section>
					<h4>Which types of mixed reality produce the widest input spaces?</h4>
					<p>Fitness apps, navigation apps, measuring apps, translation apps, responsive decorative apps</p>
					<p>(things that map the real world to a continuous output-space)</p>
				</section>
				<section>
					<h4>Which types of mixed reality produce the longest variable dwell time?</h4>
					<p>Unsure&mdash;depends on nonintrusive hardware, 5G connection, and good battery life.</p>
				</section>
				<section>
					<h4>translating this to design requirements</h4>
					<p>&lsquo;I want to turn my [real-world event] into a virtual event. What are my user stories?&rsquo;</p>
				</section>
				<section>
					<h2>1: </h2>
					<p>I want to map the content I see to a real-world input space.</p>
					<p><small>Instead of creating a static display of art on a flat surface, could the displays be curated based on current room size?</small></p>
				</section>
				<section>
					<h2>2: </h2>
					<p>I want to act on the content in a broad, body-based manner.</p>
					<p><small>Can my age, gender body type, ability of movement, etc. affect the way I interact with the interface?</small></p>
				</section>
				<section>
					<h2>3: </h2>
					<p>[still working on it]</p>
				</section>
				<section>
					<h4>questions?</h4>
				</section>
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
			});
		</script>
	</body>
</html>
